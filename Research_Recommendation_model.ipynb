{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-OJe7KBeWJN",
        "outputId": "d5d31ccd-2393-4240-8245-81e43d195a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For VAULT\n",
        "# %sh\n",
        "# /databricks/python3/bin/python3 -m spacy download en_core_web_sm\n",
        "# /databricks/python3/bin/python3 -m spacy download en_core_web_lg\n",
        "# /databricks/python/bin/pip install nltk\n",
        "# /databricks/python/bin/python -m nltk.downloader punkt"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "33945098-559d-4d67-aa20-c9c28c8fcb0a"
        },
        "id": "FOTwNuDVeQwV",
        "outputId": "5a2b7a3f-847e-4d71-ec42-11287859bccb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "<div class=\"ansiout\">/databricks/python3/bin/python3: No module named spacy\n/databricks/python3/bin/python3: No module named spacy\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.7/site-packages (3.7)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.7/site-packages (from nltk) (4.64.1)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.7/site-packages (from nltk) (0.14.1)\nRequirement already satisfied: click in /databricks/python3/lib/python3.7/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: regex&gt;=2021.8.3 in /databricks/python3/lib/python3.7/site-packages (from nltk) (2022.10.31)\nRequirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /databricks/python3/lib/python3.7/site-packages (from click-&gt;nltk) (5.0.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.4; python_version &lt; &#34;3.8&#34; in /databricks/python3/lib/python3.7/site-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;click-&gt;nltk) (4.4.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.7/site-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;click-&gt;nltk) (3.10.0)\nWARNING: You are using pip version 20.0.2; however, version 22.3 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python3.7 -m pip install --upgrade pip&#39; command.\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n</div>",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "html",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python3/bin/python3: No module named spacy\n/databricks/python3/bin/python3: No module named spacy\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.7/site-packages (3.7)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.7/site-packages (from nltk) (4.64.1)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.7/site-packages (from nltk) (0.14.1)\nRequirement already satisfied: click in /databricks/python3/lib/python3.7/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: regex&gt;=2021.8.3 in /databricks/python3/lib/python3.7/site-packages (from nltk) (2022.10.31)\nRequirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /databricks/python3/lib/python3.7/site-packages (from click-&gt;nltk) (5.0.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.4; python_version &lt; &#34;3.8&#34; in /databricks/python3/lib/python3.7/site-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;click-&gt;nltk) (4.4.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.7/site-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;click-&gt;nltk) (3.10.0)\nWARNING: You are using pip version 20.0.2; however, version 22.3 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python3.7 -m pip install --upgrade pip&#39; command.\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n</div>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install openpyxl\n",
        "!pip install spacy \n",
        "!pip install gensim\n",
        "!pip install docx2txt\n",
        "!pip install python-pptx\n",
        "!pip install xlrd==1.2.0\n",
        "!pip install pandas==1.5.1\n",
        "import string \n",
        "import csv\n",
        "from io import StringIO\n",
        "from pptx import Presentation\n",
        "import docx2txt\n",
        "import PyPDF2\n",
        "import spacy\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import nltk \n",
        "import re\n",
        "import openpyxl\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet\n",
        "import networkx as nx\n",
        "from networkx.algorithms.shortest_paths import weighted\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "25720ff1-c67b-40b9-85d2-828bec8ba396"
        },
        "id": "KcLrB18jeQwY",
        "outputId": "62a1c094-0959-4ba8-f08a-5e8496e3648c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (2.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.7/dist-packages (0.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.7/dist-packages (0.6.21)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx) (7.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx) (4.9.1)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.7/dist-packages (from python-pptx) (3.0.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlrd==1.2.0 in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pandas==1.5.1 (from versions: 0.1, 0.2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pandas==1.5.1\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "execution_count": 514
    },
    {
      "cell_type": "code",
      "source": [
        "class pdfReader:\n",
        "    \n",
        "    def __init__(self, file_path: str) -> str:\n",
        "        self.file_path = file_path\n",
        "    \n",
        "    def PDF_one_pager(self) -> str:\n",
        "        \"\"\"A function that accepts a file path to a pdf\n",
        "            as input and returns a one line string of the \n",
        "            pdf.\n",
        "            \n",
        "            Parameters:\n",
        "            file_path(str): The file path to the pdf.\n",
        "            \n",
        "            Returns:\n",
        "            one_page_pdf (str): A one line string of the pdf.\n",
        "        \n",
        "        \"\"\"\n",
        "        content = \"\"\n",
        "        p = open(self.file_path, \"rb\")\n",
        "        pdf = PyPDF2.PdfFileReader(p)\n",
        "        num_pages = pdf.numPages\n",
        "        for i in range(0, num_pages):\n",
        "            content += pdf.getPage(i).extractText() + \"\\n\"\n",
        "        content = \" \".join(content.replace(u\"\\xa0\", \" \").strip().split())\n",
        "        page_number_removal = r\"\\d{1,3} of \\d{1,3}\"\n",
        "        page_number_removal_pattern = re.compile(page_number_removal, re.IGNORECASE)\n",
        "        content = re.sub(page_number_removal_pattern, '',content)\n",
        "        \n",
        "        return content\n",
        "  \n",
        "    def pdf_reader(self) -> str:\n",
        "        \"\"\"A function that can read .pdf formatted files \n",
        "            and returns a python readable pdf.\n",
        "            \n",
        "            Parameters:\n",
        "            self (obj): An object of the class IntelPdfReader\n",
        "            \n",
        "            Returns:\n",
        "            read_pdf: A python readable .pdf file.\n",
        "        \"\"\"\n",
        "        opener = open(self.file_path,'rb')\n",
        "        read_pdf = PyPDF2.PdfFileReader(opener)\n",
        "    \n",
        "        return read_pdf\n",
        "  \n",
        "  \n",
        "    def pdf_info(self) -> dict:\n",
        "        \"\"\"A function which returns an information dictionary\n",
        "        of an object associated with the IntelPdfReader class.\n",
        "        \n",
        "        Parameters:\n",
        "        self (obj): An object of the IntelPdfReader class.\n",
        "        \n",
        "        Returns:\n",
        "        dict(pdf_info_dict): A dictionary containing the meta\n",
        "        data of the object.\n",
        "        \"\"\"\n",
        "        opener = open(self.file_path,'rb')\n",
        "        read_pdf = PyPDF2.PdfFileReader(opener)\n",
        "        pdf_info_dict = {}\n",
        "        for key,value in read_pdf.documentInfo.items():\n",
        "            pdf_info_dict[re.sub('/',\"\",key)] = value\n",
        "        return pdf_info_dict\n",
        "  \n",
        "    def pdf_dictionary(self) -> dict:\n",
        "        \"\"\"A function which returns a dictionary of \n",
        "            the object where the keys are the pages\n",
        "            and the text within the pages are the \n",
        "            values.\n",
        "            \n",
        "            Parameters:\n",
        "            obj (self): An object of the IntelPdfReader class.\n",
        "            \n",
        "            Returns:\n",
        "            dict(pdf_dict): A dictionary of the object within the\n",
        "            IntelPdfReader class.\n",
        "        \"\"\"\n",
        "        opener = open(self.file_path,'rb')\n",
        "        #try:\n",
        "        #    file_path = os.path.exists(self.file_path)\n",
        "        #    file_path = True\n",
        "        #break\n",
        "        #except ValueError:\n",
        "        #   print('Unidentifiable file path')\n",
        "        read_pdf = PyPDF2.PdfFileReader(opener)\n",
        "        length = read_pdf.numPages\n",
        "        pdf_dict = {}\n",
        "        for i in range(length):\n",
        "            page = read_pdf.getPage(i)\n",
        "            text = page.extract_text()\n",
        "            pdf_dict[i] = text\n",
        "            return pdf_dict"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5ee059ae-1c48-4975-8ee8-42ad24b24201"
        },
        "id": "Lu7KcwuOeQwZ"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "## IN PROGRESS ##\n",
        "\n",
        "class xlsxReader:\n",
        "    \n",
        "    def __init__(self, file_path: str) -> str:\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def xlsx_text(self):\n",
        "      inputExcelFile = self.file_path\n",
        "      text = str()\n",
        "      wb = openpyxl.load_workbook(inputExcelFile)\n",
        "      for ws in wb.worksheets:\n",
        "        for val in ws.values:\n",
        "          print(val)\n",
        "      for sn in wb.sheetnames:\n",
        "        print(sn)\n",
        "        excelFile = pd.read_excel(inputExcelFile, engine = 'openpyxl', sheet_name = sn)\n",
        "        excelFile.to_csv(\"ResultCsvFile.csv\", index = None, header=True)\n",
        "\n",
        "        with open(\"ResultCsvFile.csv\", \"r\") as csvFile: \n",
        "          lines = csvFile.read().split(\",\") # \"\\r\\n\" if needed\n",
        "          for val in lines:\n",
        "            if val != '':\n",
        "              text += val + ' '\n",
        "          text = text.replace('\\ufeff', '')\n",
        "          text = text.replace('\\n', ' ')\n",
        "      return text\n",
        "      "
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "38bb949c-33cd-4a8e-bf5c-446b46f4fcbc"
        },
        "id": "X2g_2fw5eQwa"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "class csvReader:\n",
        "    \n",
        "    def __init__(self, file_path: str) -> str:\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def csv_text(self):\n",
        "      text = str()\n",
        "      with open(self.file_path, \"r\") as csvFile: \n",
        "        lines = csvFile.read().split(\",\") # \"\\r\\n\" if needed\n",
        "        for val in lines:\n",
        "          text += val + ' '\n",
        "        text = text.replace('\\ufeff', '')\n",
        "        text = text.replace('\\n', ' ')\n",
        "      return text"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2945e1b4-652d-4b09-bd70-f7b059945ae6"
        },
        "id": "_nPKJTjheQwb"
      },
      "outputs": [],
      "execution_count": 62
    },
    {
      "cell_type": "code",
      "source": [
        "class pptReader:\n",
        "    \n",
        "    def __init__(self, file_path: str) -> str:\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def ppt_text(self):\n",
        "      prs = Presentation(self.file_path)\n",
        "      text = str()\n",
        "      for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "          if not shape.has_text_frame:\n",
        "              continue\n",
        "          for paragraph in shape.text_frame.paragraphs:\n",
        "            for run in paragraph.runs:\n",
        "              text += ' ' + run.text\n",
        "                  \n",
        "      return text\n",
        "    "
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e50a4cb6-ee7a-48ec-91ca-3fea76dab132"
        },
        "id": "YzPv3zo6eQwc"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "class wordDocReader:\n",
        "  def __init__(self, file_path: str) -> str:\n",
        "    self.file_path = file_path\n",
        "    \n",
        "  def word_reader(self):\n",
        "    text = docx2txt.process(self.file_path)\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = text.replace('\\t', ' ')\n",
        "    return text "
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b03dcbd4-a161-4834-9358-d2089e59077b"
        },
        "id": "f-hFzgfpeQwc"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "class dataprocessor:\n",
        "  def __init__(self):\n",
        "    return\n",
        "  \n",
        "  @staticmethod\n",
        "  def get_wordnet_pos(text: str) -> str:\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([text])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "  \n",
        "  @staticmethod\n",
        "  def preprocess(text: str):\n",
        "    #lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    #punctuation removal\n",
        "    text = \"\".join([i for i in text if i not in string.punctuation])\n",
        "    \n",
        "    #Digit removal (Only for ALL numeric numbers)\n",
        "    text = [x for x in text.split(' ') if x.isnumeric() == False]\n",
        "    \n",
        "    #Stop removal\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    custom_stopwords = ['\\n','\\n\\n', '&amp;', ' ', '.', '-', '$', '@']\n",
        "    stopwords.extend(custom_stopwords)\n",
        "\n",
        "    text = [i for i in text if i not in stopwords]\n",
        "    text = ' '.join(word for word in text)\n",
        "    \n",
        "    #lemmanization\n",
        "    lm = WordNetLemmatizer()\n",
        "    text = [lm.lemmatize(word, preprocessor.get_wordnet_pos(word)) for word in text.split(' ')]\n",
        "    text = ' '.join(word for word in text)\n",
        "    \n",
        "    text = re.sub(' +', ' ',text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def data_reader(list_file_names):\n",
        "    text_list = []\n",
        "    reader = dataprocessor()\n",
        "    for file in list_file_names:\n",
        "      temp = file.split('.')\n",
        "      filetype = temp[-1]\n",
        "      if filetype == \"pdf\":\n",
        "        file_pdf = pdfReader(file)\n",
        "        text = file_pdf.PDF_one_pager()\n",
        "\n",
        "      elif filetype == \"docx\":\n",
        "        word_doc_reader = wordDocReader(file)\n",
        "        text = word_doc_reader.word_reader()\n",
        "\n",
        "      elif filetype == \"pptx\":\n",
        "        ppt_reader = pptReader(file)\n",
        "        text = ppt_reader.ppt_text()\n",
        "        \n",
        "      elif filetype == \"csv\":\n",
        "        csv_reader = csvReader(file)\n",
        "        text = csv_reader.csv_text()\n",
        "      else:\n",
        "        print('File type {} not supported!'.format(filetype))\n",
        "        continue\n",
        "      \n",
        "      text = reader.preprocess(text)\n",
        "      text_list.append(text)\n",
        "      file_dict = dict()\n",
        "      for i,file in enumerate(list_file_names):\n",
        "        file_dict[i] = (file, file.split('/')[-1])\n",
        "    return text_list, file_dict\n",
        "\n",
        "  @staticmethod\n",
        "  def database_processor(file_dict,text_list: list) -> list:\n",
        "    file_vector_dict = dict()\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform(text_list)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    matrix = vectors.todense()\n",
        "    list_dense = matrix.tolist()\n",
        "    for i in range(len(list_dense)):\n",
        "      file_vector_dict[file_dict[i][1]] = list_dense[i]\n",
        "\n",
        "    return list_dense, vectorizer, file_vector_dict\n",
        "\n",
        "  @staticmethod\n",
        "  def input_processor(text: str, TDIF_vectorizor) -> str:\n",
        "    words = ''\n",
        "    total_words = len(text.split(' '))\n",
        "    for word in test.split(' '):\n",
        "      words += (word + ' ') * total_words\n",
        "      total_words -= 1\n",
        "\n",
        "    words = [words[:-1]]\n",
        "    words = TDIF_vectorizor.transform(words)\n",
        "    words = words.todense()\n",
        "    words = words.tolist()\n",
        "    return words\n",
        "\n",
        "  @staticmethod\n",
        "  def similarity_checker(vector_1: int, vector_2: int ):\n",
        "    vectors = [vector_1, vector_2]\n",
        "    for vec in vectors:\n",
        "      if np.ndim(vec) == 1:\n",
        "        vec = np.expand_dims(vec, axis=0)\n",
        "    return cosine_similarity([vector_1], vector_2)\n",
        "\n",
        "  @staticmethod\n",
        "  def recommender(vector_file_list,query_vector, file_dict):\n",
        "    similarity_list = []\n",
        "    score_dict = dict()\n",
        "    for i,file_vector in enumerate(vector_file_list):\n",
        "      x = dataprocessor.similarity_checker(file_vector, query_vector)\n",
        "      score_dict[file_dict[i][1]] = (x[0][0])\n",
        "      similarity_list.append(x)\n",
        "    similarity_list = sorted(similarity_list, reverse = True)\n",
        "    #Recommends the top 20%\n",
        "    recommended = sorted(score_dict.items(), key=lambda x:-x[1])[:int(np.round(.2*len(similarity_list)))]\n",
        "  \n",
        "    final_recommendation = []\n",
        "    for i in range(len(recommended)):\n",
        "      final_recommendation.append(recommended[i][0])\n",
        "    #add in graph for greater than 3 recommendationa\n",
        "    return final_recommendation, similarity_list[:len(final_recommendation)]\n",
        "\n",
        "  @staticmethod\n",
        "  def page_ranker(recommendation_val, file_vec_dict):\n",
        "    my_graph = nx.Graph()\n",
        "    for i in range(len(recommendation_val)):\n",
        "      file_1 = recommendation_val[i]\n",
        "      for j in range(len(recommendation_val)):\n",
        "        file_2 = recommendation_val[j]\n",
        "\n",
        "        if i != j:\n",
        "          #Calculate sim_score between two values (weight)\n",
        "          edge_dist = cosine_similarity([file_vec_dict[recommendation_val[i]]],[file_vec_dict[recommendation_val[j]]])\n",
        "          #add an edge from file 1 to file 2 with the weight \n",
        "          my_graph.add_edge(file_1, file_2, weight=edge_dist)\n",
        "\n",
        "    #Pagerank the graph      \n",
        "    rec = nx.algorithms.link_analysis.pagerank_numpy(my_graph, weight='weight')\n",
        "    #Takes 20% of the values\n",
        "    pr_recommended = sorted(rec.items(), key=lambda x:-x[1])[:int(np.round(len(rec)))]\n",
        "\n",
        "    return pr_recommended\n",
        "\n",
        "  @staticmethod\n",
        "  def weighted_final_rank(similarity_list,pr_recommended,final_recommendation):\n",
        "\n",
        "    final_dict = dict()\n",
        "\n",
        "    for i in range(len(sim_list)):\n",
        "      val = (.8*sim_list[final_recommendation.index(pr_recommendation[i][0])].squeeze()) + (.2 * pr_recommendation[i][1])\n",
        "      final_dict[pr_recommendation[i][0]] = val\n",
        "\n",
        "    weighted_final_recommend = sorted(final_dict.items(), key=lambda x:-x[1])[:int(np.round(len(final_dict)))]\n",
        "\n",
        "    return weighted_final_recommend"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "97c95481-0e69-4482-bf0a-ebf2b1015838"
        },
        "id": "3IFLzsvyeQwd"
      },
      "outputs": [],
      "execution_count": 525
    },
    {
      "cell_type": "code",
      "source": [
        "#r_0 - NAVAIR Dataset/original data files/Appendix G. EOSTATE Reports/A011 Controlling Report.pdf'\n",
        "#r_1 - TTCP/TTCP US DATA COLLECTION/TTCP US Defect Analysis/CT Results/us 39 vertical.csv\n",
        "#r_2 - TTCP/TTCP US DATA COLLECTION/TTCP US Build Files/US Build Summary TTCP AM CP Distro D.docx'\n",
        "#r_3 - TTCP/TTCP US DATA COLLECTION/TTCP US Build Files/TTCP_Build_Information.pptx\n",
        "\n",
        "\n",
        "#r_0 = '/dbfs/FileStore/shared_uploads/1523471828.MCCLOSKEY/A011_Controlling_Report.pdf'\n",
        "r_0 = '/content/gdrive/MyDrive/Hackathon_22_1/NAVAIR Dataset/original data files/Appendix G. EOSTATE Reports/A011 Controlling Report.pdf'\n",
        "\n",
        "\n",
        "# r_2 = '/dbfs/FileStore/shared_uploads/1523471828.MCCLOSKEY/us_39_vertical.csv'\n",
        "# r_1 ='/dbfs/FileStore/shared_uploads/1523471828.MCCLOSKEY/us_39_vertical_hist.csv'\n",
        "\n",
        "r_1 = '/content/gdrive/MyDrive/Hackathon_22_1/TTCP/TTCP US DATA COLLECTION/TTCP US Defect Analysis/CT Results/us 39 vertical.csv'\n",
        "\n",
        "#r_2 = '/dbfs/FileStore/shared_uploads/1523471828.MCCLOSKEY/US_Build_Summary_TTCP_AM_CP_Distro_D.docx'\n",
        "r_2 = '/content/gdrive/MyDrive/Hackathon_22_1/TTCP/TTCP US DATA COLLECTION/TTCP US Build Files/US Build Summary TTCP AM CP Distro D.docx'\n",
        "\n",
        "#r_3 = '/dbfs/FileStore/shared_uploads/1523471828.MCCLOSKEY/TTCP_Build_Information.pptx'\n",
        "r_3 = '/content/gdrive/MyDrive/Hackathon_22_1/TTCP/TTCP US DATA COLLECTION/TTCP US Build Files/TTCP_Build_Information.pptx'\n",
        "\n",
        "\n",
        "directory = [r_0, r_1, r_2, r_3]\n",
        "research_documents, file_dictionary = dataprocessor.data_reader(directory)\n",
        "list_files, vectorizer, file_vec_dict = dataprocessor.database_processor(file_dictionary,research_documents)\n",
        "\n",
        "query = '.5 tolerance level of oxygen boosters'\n",
        "query = dataprocessor.preprocess(query)\n",
        "query = dataprocessor.input_processor(query, vectorizer)\n",
        "recommendation, sim_list = dataprocessor.recommender(list_files,query, file_dictionary)\n",
        "print('Files to Reference: {}'. format(recommendation))\n",
        "pr_recommendation = dataprocessor.page_ranker(recommendation, file_vec_dict)\n",
        "final_weighted_recommended = dataprocessor.weighted_final_rank(sim_list,pr_recommendation,  recommendation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKorTVm-pqAy",
        "outputId": "7bb17b35-cb80-4001-a644-58d6af5b621b"
      },
      "execution_count": 526,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files to Reference: ['us 39 vertical.csv', 'US Build Summary TTCP AM CP Distro D.docx']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:147: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_weighted_recommended"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm_Nvue5jIQd",
        "outputId": "45003925-19c6-4fd9-dfd2-31c4b2f804e9"
      },
      "execution_count": 527,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('us 39 vertical.csv', 0.7508569953985282),\n",
              " ('US Build Summary TTCP AM CP Distro D.docx', 0.11025390932169342)]"
            ]
          },
          "metadata": {},
          "execution_count": 527
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwCmz1y4_h7B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "whose_gonna_carry_the_boats",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 888920
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}